\section{Einleitung}
Die Entwicklung von Deep Learning Modellen führte in den letzten Jahren zu signifikanten Fortschritten in der Bildklassifikation. Jedoch zeigen Studien \cite{szegedy_intriguing_2014}\cite{goodfellow_explaining_2015}, dass diese Modelle anfällig für sogenannte Adversarial Attacks sein können. Eine besondere Form solcher Angriffe sind Universal Adversarial Perturbations (\acrshort{uap}s) \cite{moosavi-dezfooli_universal_2017}, gezielt erzeugtes Rauschen, das auf verschiedene Eingabebilder addiert werden kann, um Fehlklassifikationen zu verursachen. Die vorliegende Thesis untersucht die Auswirkungen von \acrshort{uap}s auf Deep Learning Modelle im Kontext der binären Klassifikation medizinischer Bilder sowie Methoden zur Erhöhung der Robustheit dieser Modelle.

Das Hauptziel dieser Thesis ist, die Robustheit von Deep Learning Modellen gegenüber \acrshort{uap}s zu untersuchen und zu verbessern. Dabei werden insbesondere die Modelle ResNet, DenseNet und EfficientNet analysiert. Ein weiteres Ziel ist die Entwicklung und Implementierung eines anwendungsspezifischen Algorithmus zur Generierung von \acrshort{uap}s sowie die Evaluierung von Adversarial Training als Verteidigungsmechanismus.

Zur Erreichung dieser Ziele wird ein mehrstufiger Ansatz verfolgt. Zunächst werden die genannten Modelle auf zwei medizinischen Datensätzen (COVIDx CXR-4 und Brain Tumor) trainiert. Anschliessend wird ein angepasster Algorithmus zur Generierung von \acrshort{uap}s implementiert, der auf die Erhöhung der Falsch-Negativ-Rate abzielt. Die Robustheit der Modelle wird dann durch wiederholtes Adversarial Training verbessert. Für die technische Umsetzung kommen die Deep Learning Frameworks PyTorch und PyTorch-Lightning zum Einsatz, während der Trainingsprozess mit Weights \& Biases überwacht wird.

Die Untersuchungen zeigen, dass \acrshort{uap}s in der Lage sind, die Leistung der Modelle signifikant zu beeinträchtigen, insbesondere hinsichtlich der Recall-Metriken. Obwohl Adversarial Training die Robustheit der Modelle verbessern kann, bleiben diese weiterhin anfällig für neu generierte \acrshort{uap}s. Eine wichtige Erkenntnis ist, dass die \Gls{robustifizierung} in einigen Fällen dazu führt, dass grössere Perturbationen erforderlich sind, um die Modelle zu täuschen, was die Zeitintensität der \acrshort{uap}-Generierung erhöht.

Die vorliegende Thesis ist wie folgt strukturiert: Nach dieser Einleitung werden in Kapitel 2 die theoretischen Grundlagen von Adversarial Attacks und \acrshort{uap}s erläutert. Kapitel 3 beschreibt die verwendeten Datensätze. In Kapitel 4 wird die angewandte Methodik detailliert dargestellt, einschliesslich der Modellarchitekturen, des \acrshort{uap}-Algorithmus und des Adversarial Trainings. Kapitel 5 präsentiert und interpretiert die Ergebnisse der Untersuchungen, während Kapitel 6 diese diskutiert und einen Ausblick auf zukünftige Forschungsrichtungen gibt.