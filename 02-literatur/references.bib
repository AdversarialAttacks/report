
@misc{dosovitskiy_image_2021,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	number = {{arXiv}:2010.11929},
	publisher = {{arXiv}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2024-03-19},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{tan_efficientnetv2_2021,
	title = {{EfficientNetV}2: Smaller Models and Faster Training},
	url = {http://arxiv.org/abs/2104.00298},
	doi = {10.48550/arXiv.2104.00298},
	shorttitle = {{EfficientNetV}2},
	abstract = {This paper introduces {EfficientNetV}2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-{MBConv}. Our experiments show that {EfficientNetV}2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our {EfficientNetV}2 significantly outperforms previous models on {ImageNet} and {CIFAR}/Cars/Flowers datasets. By pretraining on the same {ImageNet}21k, our {EfficientNetV}2 achieves 87.3\% top-1 accuracy on {ImageNet} {ILSVRC}2012, outperforming the recent {ViT} by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
	number = {{arXiv}:2104.00298},
	publisher = {{arXiv}},
	author = {Tan, Mingxing and Le, Quoc V.},
	urldate = {2024-03-19},
	date = {2021-06-23},
	eprinttype = {arxiv},
	eprint = {2104.00298 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{akhtar_advances_2021,
	title = {Advances in adversarial attacks and defenses in computer vision: A survey},
	url = {http://arxiv.org/abs/2108.00401},
	shorttitle = {Advances in adversarial attacks and defenses in computer vision},
	abstract = {Deep Learning ({DL}) is the most widely used tool in the contemporary field of computer vision. Its ability to accurately solve complex problems is employed in vision research to learn deep neural models for a variety of tasks, including security critical applications. However, it is now known that {DL} is vulnerable to adversarial attacks that can manipulate its predictions by introducing visually imperceptible perturbations in images and videos. Since the discovery of this phenomenon in 2013{\textasciitilde}[1], it has attracted significant attention of researchers from multiple sub-fields of machine intelligence. In [2], we reviewed the contributions made by the computer vision community in adversarial attacks on deep learning (and their defenses) until the advent of year 2018. Many of those contributions have inspired new directions in this area, which has matured significantly since witnessing the first generation methods. Hence, as a legacy sequel of [2], this literature review focuses on the advances in this area since 2018. To ensure authenticity, we mainly consider peer-reviewed contributions published in the prestigious sources of computer vision and machine learning research. Besides a comprehensive literature review, the article also provides concise definitions of technical terminologies for non-experts in this domain. Finally, this article discusses challenges and future outlook of this direction based on the literature reviewed herein and [2].},
	number = {{arXiv}:2108.00401},
	publisher = {{arXiv}},
	author = {Akhtar, Naveed and Mian, Ajmal and Kardan, Navid and Shah, Mubarak},
	urldate = {2024-03-14},
	date = {2021-09-02},
	eprinttype = {arxiv},
	eprint = {2108.00401 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{bhuvaji_brain_2020,
	title = {Brain Tumor Classification ({MRI})},
	url = {https://www.kaggle.com/dsv/1183165},
	publisher = {Kaggle},
	author = {Bhuvaji, Sartaj and Kadam, Ankita and Bhumkar, Prajakta and Dedge, Sameer and Kanchan, Swati},
	date = {2020},
	doi = {10.34740/KAGGLE/DSV/1183165},
}

@misc{wu_covidx_2023,
	title = {{COVIDx} {CXR}-4: An Expanded Multi-Institutional Open-Source Benchmark Dataset for Chest X-ray Image-Based Computer-Aided {COVID}-19 Diagnostics},
	url = {http://arxiv.org/abs/2311.17677},
	shorttitle = {{COVIDx} {CXR}-4},
	abstract = {The global ramifications of the {COVID}-19 pandemic remain significant, exerting persistent pressure on nations even three years after its initial outbreak. Deep learning models have shown promise in improving {COVID}-19 diagnostics but require diverse and larger-scale datasets to improve performance. In this paper, we introduce {COVIDx} {CXR}-4, an expanded multi-institutional open-source benchmark dataset for chest X-ray image-based computer-aided {COVID}-19 diagnostics. {COVIDx} {CXR}-4 expands significantly on the previous {COVIDx} {CXR}-3 dataset by increasing the total patient cohort size by greater than 2.66 times, resulting in 84,818 images from 45,342 patients across multiple institutions. We provide extensive analysis on the diversity of the patient demographic, imaging metadata, and disease distributions to highlight potential dataset biases. To the best of the authors' knowledge, {COVIDx} {CXR}-4 is the largest and most diverse open-source {COVID}-19 {CXR} dataset and is made publicly available as part of an open initiative to advance research to aid clinicians against the {COVID}-19 disease.},
	number = {{arXiv}:2311.17677},
	publisher = {{arXiv}},
	author = {Wu, Yifan and Gunraj, Hayden and Tai, Chi-en Amy and Wong, Alexander},
	urldate = {2024-03-14},
	date = {2023-11-29},
	eprinttype = {arxiv},
	eprint = {2311.17677 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{huang_densely_2018,
	title = {Densely Connected Convolutional Networks},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network ({DenseNet}), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. {DenseNets} have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks ({CIFAR}-10, {CIFAR}-100, {SVHN}, and {ImageNet}). {DenseNets} obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/{DenseNet} .},
	number = {{arXiv}:1608.06993},
	publisher = {{arXiv}},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	urldate = {2024-03-14},
	date = {2018-01-28},
	eprinttype = {arxiv},
	eprint = {1608.06993 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	urldate = {2024-03-14},
	date = {2012},
}

@misc{szegedy_going_2014,
	title = {Going Deeper with Convolutions},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC} 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC} 2014 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	number = {{arXiv}:1409.4842},
	publisher = {{arXiv}},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2024-03-14},
	date = {2014-09-16},
	eprinttype = {arxiv},
	eprint = {1409.4842 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	number = {{arXiv}:1512.03385},
	publisher = {{arXiv}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2024-03-14},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{iandola_squeezenet_2016,
	title = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and {\textless}0.5MB model size},
	url = {http://arxiv.org/abs/1602.07360},
	shorttitle = {{SqueezeNet}},
	abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple {DNN} architectures that achieve that accuracy level. With equivalent accuracy, smaller {DNN} architectures offer at least three advantages: (1) Smaller {DNNs} require less communication across servers during distributed training. (2) Smaller {DNNs} require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller {DNNs} are more feasible to deploy on {FPGAs} and other hardware with limited memory. To provide all of these advantages, we propose a small {DNN} architecture called {SqueezeNet}. {SqueezeNet} achieves {AlexNet}-level accuracy on {ImageNet} with 50x fewer parameters. Additionally, with model compression techniques we are able to compress {SqueezeNet} to less than 0.5MB (510x smaller than {AlexNet}). The {SqueezeNet} architecture is available for download here: https://github.com/{DeepScale}/{SqueezeNet}},
	number = {{arXiv}:1602.07360},
	publisher = {{arXiv}},
	author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	urldate = {2024-03-14},
	date = {2016-11-04},
	eprinttype = {arxiv},
	eprint = {1602.07360 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {{arXiv}:1409.1556},
	publisher = {{arXiv}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2024-03-14},
	date = {2015-04-10},
	eprinttype = {arxiv},
	eprint = {1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{li_review_2022,
	title = {A Review of Adversarial Attack and Defense for Classification Methods},
	volume = {76},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2021.2006781},
	doi = {10.1080/00031305.2021.2006781},
	abstract = {Despite the efficiency and scalability of machine learning systems, recent studies have demonstrated that many classification methods, especially Deep Neural Networks ({DNNs}), are vulnerable to adversarial examples; that is, examples that are carefully crafted to fool a well-trained classification model while being indistinguishable from natural data to human. This makes it potentially unsafe to apply {DNNs} or related methods in security-critical areas. Since this issue was first identified by Biggio et al. and Szegedy et al., much work has been done in this field, including the development of attack methods to generate adversarial examples and the construction of defense techniques to guard against such examples. This article aims to introduce this topic and its latest developments to the statistical community, primarily focusing on the generation and guarding of adversarial examples. Computing codes (in Python and R) used in the numerical experiments are publicly available for readers to explore the surveyed methods. It is the hope of the authors that this article will encourage more statisticians to work on this important and exciting field of generating and defending against adversarial examples.},
	pages = {329--345},
	number = {4},
	journaltitle = {The American Statistician},
	author = {Li, Yao and Cheng, Minhao and Hsieh, Cho-Jui and Lee, Thomas C. M.},
	urldate = {2024-03-12},
	date = {2022-10-02},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2021.2006781},
	keywords = {Adversarial examples, Adversarial training, Deep neural networks, Defense robustness},
}

@article{amirian_trace_2018,
	title = {Trace and Detect Adversarial Attacks on {CNNs} using Feature Response Maps},
	url = {http://arxiv.org/abs/2208.11436},
	doi = {10.21256/zhaw-3863},
	abstract = {The existence of adversarial attacks on convolutional neural networks ({CNN}) questions the fitness of such models for serious applications. The attacks manipulate an input image such that misclassification is evoked while still looking normal to a human observer -- they are thus not easily detectable. In a different context, backpropagated activations of {CNN} hidden layers -- "feature responses" to a given input -- have been helpful to visualize for a human "debugger" what the {CNN} "looks at" while computing its output. In this work, we propose a novel detection method for adversarial examples to prevent attacks. We do so by tracking adversarial perturbations in feature responses, allowing for automatic detection using average local spatial entropy. The method does not alter the original network architecture and is fully human-interpretable. Experiments confirm the validity of our approach for state-of-the-art attacks on large-scale models trained on {ImageNet}.},
	author = {Amirian, Mohammadreza and Schwenker, Friedhelm and Stadelmann, Thilo},
	urldate = {2024-03-07},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {2208.11436 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xu_robust_2023,
	title = {Robust Universal Adversarial Perturbations},
	url = {http://arxiv.org/abs/2206.10858},
	abstract = {Universal Adversarial Perturbations ({UAPs}) are imperceptible, image-agnostic vectors that cause deep neural networks ({DNNs}) to misclassify inputs with high probability. In practical attack scenarios, adversarial perturbations may undergo transformations such as changes in pixel intensity, scaling, etc. before being added to {DNN} inputs. Existing methods do not create {UAPs} robust to these real-world transformations, thereby limiting their applicability in practical attack scenarios. In this work, we introduce and formulate {UAPs} robust against real-world transformations. We build an iterative algorithm using probabilistic robustness bounds and construct such {UAPs} robust to transformations generated by composing arbitrary sub-differentiable transformation functions. We perform an extensive evaluation on the popular {CIFAR}-10 and {ILSVRC} 2012 datasets measuring our {UAPs}' robustness under a wide range common, real-world transformations such as rotation, contrast changes, etc. We further show that by using a set of primitive transformations our method can generalize well to unseen transformations such as fog, {JPEG} compression, etc. Our results show that our method can generate {UAPs} up to 23\% more robust than state-of-the-art baselines.},
	number = {{arXiv}:2206.10858},
	publisher = {{arXiv}},
	author = {Xu, Changming and Singh, Gagandeep},
	urldate = {2024-03-07},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2206.10858 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{moosavi-dezfooli_universal_2017,
	title = {Universal adversarial perturbations},
	url = {http://arxiv.org/abs/1610.08401},
	abstract = {Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.},
	number = {{arXiv}:1610.08401},
	publisher = {{arXiv}},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	urldate = {2024-03-07},
	date = {2017-03-09},
	eprinttype = {arxiv},
	eprint = {1610.08401 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sarker_deep_2021,
	title = {Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions},
	volume = {2},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-021-00815-1},
	doi = {10.1007/s42979-021-00815-1},
	shorttitle = {Deep Learning},
	abstract = {Deep learning ({DL}), a branch of machine learning ({ML}) and artificial intelligence ({AI}) is nowadays considered as a core technology of today’s Fourth Industrial Revolution (4IR or Industry 4.0). Due to its learning capabilities from data, {DL} technology originated from artificial neural network ({ANN}), has become a hot topic in the context of computing, and is widely applied in various application areas like healthcare, visual recognition, text analytics, cybersecurity, and many more. However, building an appropriate {DL} model is a challenging task, due to the dynamic nature and variations in real-world problems and data. Moreover, the lack of core understanding turns {DL} methods into black-box machines that hamper development at the standard level. This article presents a structured and comprehensive view on {DL} techniques including a taxonomy considering various types of real-world tasks like supervised or unsupervised. In our taxonomy, we take into account deep networks for supervised or discriminative learning, unsupervised or generative learning as well as hybrid learning and relevant others. We also summarize real-world application areas where deep learning techniques can be used. Finally, we point out ten potential aspects for future generation {DL} modeling with research directions. Overall, this article aims to draw a big picture on {DL} modeling that can be used as a reference guide for both academia and industry professionals.},
	pages = {420},
	number = {6},
	journaltitle = {{SN} Computer Science},
	shortjournal = {{SN} {COMPUT}. {SCI}.},
	author = {Sarker, Iqbal H.},
	urldate = {2024-02-26},
	date = {2021-08-18},
	langid = {english},
	keywords = {Artificial intelligence, Artificial neural network, Deep learning, Discriminative learning, Generative learning, Hybrid learning, Intelligent systems},
}

@misc{strauss_ensemble_2018,
	title = {Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks},
	url = {http://arxiv.org/abs/1709.03423},
	abstract = {Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the {MNIST} and the {CIFAR}-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.},
	number = {{arXiv}:1709.03423},
	publisher = {{arXiv}},
	author = {Strauss, Thilo and Hanselmann, Markus and Junginger, Andrej and Ulmer, Holger},
	urldate = {2024-02-26},
	date = {2018-02-08},
	eprinttype = {arxiv},
	eprint = {1709.03423 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{su_one_2019,
	title = {One pixel attack for fooling deep neural networks},
	volume = {23},
	issn = {1089-778X, 1089-778X, 1941-0026},
	url = {http://arxiv.org/abs/1710.08864},
	doi = {10.1109/TEVC.2019.2890858},
	abstract = {Recent research has revealed that the output of Deep Neural Networks ({DNN}) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution ({DE}). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of {DE}. The results show that 67.97\% of the natural images in Kaggle {CIFAR}-10 test dataset and 16.04\% of the {ImageNet} ({ILSVRC} 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03\% and 22.91\% confidence on average. We also show the same vulnerability on the original {CIFAR}-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current {DNNs} are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of {DE} (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
	pages = {828--841},
	number = {5},
	journaltitle = {{IEEE} Transactions on Evolutionary Computation},
	shortjournal = {{IEEE} Trans. Evol. Computat.},
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	urldate = {2024-02-21},
	date = {2019-10},
	eprinttype = {arxiv},
	eprint = {1710.08864 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	number = {{arXiv}:1312.6199},
	publisher = {{arXiv}},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	urldate = {2024-02-21},
	date = {2014-02-19},
	eprinttype = {arxiv},
	eprint = {1312.6199 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{perruchoud_24fs_i4ds27_2023,
	title = {24FS\_I4DS27: Adversarial Attacks - Wie kann {KI} überlistet wer- den?},
	abstract = {Was lernen neuronale Netze genau?
Wie sicher sind sie sich ihrer Vorher-
sage und wie einfach lassen sie sich
austricksen? Es stellt sich heraus,
dass es ein Leichtes ist ein neurona-
les Netz ({NN}) zu korrumpieren: ein
Bild einer vorgegebenen Klasse lässt
sich mit kleinsten Anpassungen so
verändern, dass {KI} die richtige Klasse
nicht mehr erkennt und eine komplett
falsche Klassifikation vornimmt (s. Bild).
In einer Welt, wo {ML}/{AI} mehr und mehr Einfluss auf unser Leben insbesondere in sensitiven Bereichen wie Gesundheitswesen und Finanzen nimmt, sind die Implikationen von Adversarial Attacks von enormer Trag- weite. Dieses Projekt widmet sich der Thematik Adversarial Attack und zielt darauf ab Modelle robuster zu machen.},
	publisher = {{FHNW}},
	author = {Perruchoud, Daniel and Heule, Stephan},
	date = {2023-11-12},
}

@online{madry_brief_nodate,
	title = {A Brief Introduction to Adversarial Examples},
	url = {https://gradientscience.org/intro_adversarial/},
	abstract = {Research highlights and perspectives on machine learning and optimization from {MadryLab}.},
	titleaddon = {gradient science},
	author = {Madry, Aleksander and Schmidt, Ludwig},
	urldate = {2024-02-09},
}

@misc{ilyas_adversarial_2019,
	title = {Adversarial Examples Are Not Bugs, They Are Features},
	url = {http://arxiv.org/abs/1905.02175},
	doi = {10.48550/arXiv.1905.02175},
	abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
	number = {{arXiv}:1905.02175},
	publisher = {{arXiv}},
	author = {Ilyas, Andrew and Shibani, Santurkar and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	urldate = {2024-02-09},
	date = {2019-08-12},
	eprinttype = {arxiv},
	eprint = {1905.02175 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{ilyas_adversarial_nodate,
	title = {Adversarial Examples Are Not Bugs, They Are Features},
	url = {https://gradientscience.org/adv/},
	abstract = {Research highlights and perspectives on machine learning and optimization from {MadryLab}.},
	titleaddon = {gradient science},
	author = {Ilyas, Andrew and Shibani, Santurkar and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	urldate = {2024-02-09},
}

@article{engstrom_discussion_2019,
	title = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features'},
	volume = {4},
	issn = {2476-0757},
	url = {https://distill.pub/2019/advex-bugs-discussion},
	doi = {10.23915/distill.00019},
	abstract = {Six comments from the community and responses from the original authors},
	pages = {e19},
	number = {8},
	journaltitle = {Distill},
	shortjournal = {Distill},
	author = {Engstrom, Logan and Gilmer, Justin and Goh, Gabriel and Hendrycks, Dan and Ilyas, Andrew and Madry, Aleksander and Nakano, Reiichiro and Nakkiran, Preetum and Santurkar, Shibani and Tran, Brandon and Tsipras, Dimitris and Wallace, Eric},
	urldate = {2024-02-09},
	date = {2019-08-06},
	langid = {english},
}

@misc{pintor_fast_2021,
	title = {Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints},
	url = {http://arxiv.org/abs/2102.12827},
	abstract = {Evaluating adversarial robustness amounts to finding the minimum perturbation needed to have an input sample misclassified. The inherent complexity of the underlying optimization requires current gradient-based attacks to be carefully tuned, initialized, and possibly executed for many computationally-demanding iterations, even if specialized to a given perturbation model. In this work, we overcome these limitations by proposing a fast minimum-norm ({FMN}) attack that works with different \${\textbackslash}ell\_p\$-norm perturbation models (\$p=0, 1, 2, {\textbackslash}infty\$), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. It works by iteratively finding the sample misclassified with maximum confidence within an \${\textbackslash}ell\_p\$-norm constraint of size \${\textbackslash}epsilon\$, while adapting \${\textbackslash}epsilon\$ to minimize the distance of the current sample to the decision boundary. Extensive experiments show that {FMN} significantly outperforms existing attacks in terms of convergence speed and computation time, while reporting comparable or even smaller perturbation sizes.},
	number = {{arXiv}:2102.12827},
	publisher = {{arXiv}},
	author = {Pintor, Maura and Roli, Fabio and Brendel, Wieland and Biggio, Battista},
	urldate = {2024-01-25},
	date = {2021-11-19},
	eprinttype = {arxiv},
	eprint = {2102.12827 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-based Localization},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	shorttitle = {Grad-{CAM}},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of {CNN}-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-{CAM}), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-{CAM} is applicable to a wide variety of {CNN} model-families: (1) {CNNs} with fully-connected layers, (2) {CNNs} used for structured outputs, (3) {CNNs} used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-{CAM} with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering ({VQA}) models, including {ResNet}-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and {VQA}, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-{CAM} and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-{CAM} helps users establish appropriate trust in predictions from models and show that Grad-{CAM} helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/{COjUB}9Izk6E.},
	pages = {336--359},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	urldate = {2024-01-25},
	date = {2020-02},
	eprinttype = {arxiv},
	eprint = {1610.02391 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{rony_augmented_2021,
	title = {Augmented Lagrangian Adversarial Attacks},
	url = {http://arxiv.org/abs/2011.11857},
	abstract = {Adversarial attack algorithms are dominated by penalty methods, which are slow in practice, or more efficient distance-customized methods, which are heavily tailored to the properties of the distance considered. We propose a white-box attack algorithm to generate minimally perturbed adversarial examples based on Augmented Lagrangian principles. We bring several algorithmic modifications, which have a crucial effect on performance. Our attack enjoys the generality of penalty methods and the computational efficiency of distance-customized algorithms, and can be readily used for a wide set of distances. We compare our attack to state-of-the-art methods on three datasets and several models, and consistently obtain competitive performances with similar or lower computational complexity.},
	number = {{arXiv}:2011.11857},
	publisher = {{arXiv}},
	author = {Rony, Jérôme and Granger, Eric and Pedersoli, Marco and Ayed, Ismail Ben},
	urldate = {2024-01-25},
	date = {2021-08-19},
	eprinttype = {arxiv},
	eprint = {2011.11857 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{madry_towards_2019,
	title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/{MadryLab}/mnist\_challenge and https://github.com/{MadryLab}/cifar10\_challenge.},
	number = {{arXiv}:1706.06083},
	publisher = {{arXiv}},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	urldate = {2024-01-25},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1706.06083 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{samangouei_defense-gan_2018,
	title = {Defense-{GAN}: Protecting Classifiers Against Adversarial Attacks Using Generative Models},
	url = {http://arxiv.org/abs/1805.06605},
	shorttitle = {Defense-{GAN}},
	abstract = {In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-{GAN}, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-{GAN} is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-{GAN} is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at https://github.com/kabkabm/defensegan},
	number = {{arXiv}:1805.06605},
	publisher = {{arXiv}},
	author = {Samangouei, Pouya and Kabkab, Maya and Chellappa, Rama},
	urldate = {2024-01-23},
	date = {2018-05-17},
	eprinttype = {arxiv},
	eprint = {1805.06605 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{noauthor_links_nodate,
	title = {Links und Mustervorlagen (Templates, inkl. Layout-Vorgaben)},
	url = {https://www.fhnw.ch/plattformen/academicguide/de/links-und-mustervorlagen-templates-2/},
	shorttitle = {{NICHT} {ZITTIEREN}},
	urldate = {2024-01-23},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and Harnessing Adversarial Examples},
	url = {http://arxiv.org/abs/1412.6572},
	doi = {10.48550/arXiv.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the {MNIST} dataset.},
	number = {{arXiv}:1412.6572},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	urldate = {2024-01-23},
	date = {2015-03-20},
	eprinttype = {arxiv},
	eprint = {1412.6572 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xie_adversarial_2020,
	title = {Adversarial Examples Improve Image Recognition},
	url = {http://arxiv.org/abs/1911.09665},
	abstract = {Adversarial examples are commonly viewed as a threat to {ConvNets}. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose {AdvProp}, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that {AdvProp} improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying {AdvProp} to the latest {EfficientNet}-B7 [28] on {ImageNet}, we achieve significant improvements on {ImageNet} (+0.7\%), {ImageNet}-C (+6.5\%), {ImageNet}-A (+7.0\%), Stylized-{ImageNet} (+4.8\%). With an enhanced {EfficientNet}-B8, our method achieves the state-of-the-art 85.5\% {ImageNet} top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images ({\textasciitilde}3000X more than {ImageNet}) and {\textasciitilde}9.4X more parameters. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	number = {{arXiv}:1911.09665},
	publisher = {{arXiv}},
	author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V.},
	urldate = {2024-01-23},
	date = {2020-04-14},
	eprinttype = {arxiv},
	eprint = {1911.09665 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
