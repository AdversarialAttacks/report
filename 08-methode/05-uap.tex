\subsection{UAP Algorithmus} \label{chap:UAP}

Der hier beschriebene Algorithmus für die Generation von einer \acrfull{uap} basiert auf dem Ansatz von Moosavi-Dezfooli et al. \cite{moosavi-dezfooli_universal_2017}. Ziel ist, eine Perturbation zu entwickeln, die auf neue Bilder übertragbar ist. Der Algorithmus durchläuft iterativ eine Auswahl von Trainingsbildern und findet jeweils den kürzesten Weg zur Entscheidungsgrenze jedes Bildes. Dabei werden die erzeugten Perturbationen aufsummiert, um eine universelle Perturbation zu erzeugen.

Bei der Generation der \acrshort{uap}s wird nur eine Teilmenge der positiv gelabelten Trainingsbilder verwendet. Die generierten \acrshort{uap}s sind speziell darauf ausgelegt, positive Klassenbilder fälschlicherweise als negative zu klassifizieren.

\subsubsection{Loss-Funktion $L_{\text{UAP}}$}
Das Optimierungsproblem des Referenzpapers \cite{moosavi-dezfooli_universal_2017} wird in dieser Implementierung durch eine selbst definierte Loss-Funktion gelöst, welche die Norm der Perturbationstensor und die inverse \acrlong{bce} Funktion minimiert. Durch die Minimierung der Norm der Perturbationstensor wird sichergestellt, dass die erzeugte Perturbation minimal ist und somit weniger auffällig. Gleichzeitig sorgt die Minimierung der Inverse \acrlong{bce} dafür, dass die aktuelle Perturbation an die Entscheidungsgrenze bringt. Diese doppelte Optimierung stellt sicher, dass die Perturbationen sowohl klein als auch wirksam ist.

% Die Grundidee dabei ist, dass die Norm die aktuelle Perturbation so klein wie möglich hält und die Inverse der \acrlong{bce} die aktuelle Perturbation an die Entscheidungsgrenze bringt.

Die Loss-Funktion optimiert den Tensor $\Delta v$ und sieht wie folgt aus:
\begin{equation}
    L_{UAP} = \lambda_{norm} \cdot ||v + \Delta v||_p + \frac{1}{L_{BCE}(\hat{y}, \hat{y}_{\text{adv}}) + \epsilon}
\label{Loss}
\end{equation}

Wobei die verwendete \acrlong{bce} wie folgt minimiert wird:
\begin{equation}
L_{BCE} = - (\hat{y} \cdot \log(\hat{y}_{\text{adv}}) + (1-\hat{y}) \cdot \log(1-\hat{y}_{\text{adv}}))
\label{eq:BCE}
\end{equation}

\begin{align*}
%\text{wobei:}&\\
L_{UAP}, &\text{ ist der gesamte Loss.} \\
L_{BCE}, &\text{ bezeichnet den \acrlong{bce} Loss.} \\
\lambda_{norm}, &\text{ ist der Regularisierungsparameter der Matrizennorm.} \\
v, &\text{ ist die universelle adversarial Perturbation.} \\
\Delta v, &\text{ ist die Änderung der Perturbation } v \text{.} \\
||v + \Delta v||_p, &\text{ repräsentiert die } L_p \text{-Norm der Perturbation } v + \Delta v. \\
\hat{y}, &\text{ ist die Vorhersage des Modells für das Originalbild ($\text{img}$).} \\
\hat{y}_{adv}, &\text{ ist die Vorhersage des Modells für das perturbierte Bild ($\text{img} + v + \Delta v$).} \\
\epsilon, &\text{ ist eine kleine positive Konstante für numerische Stabilität.}
\end{align*}

Der Algorithmus erfordert eine Batch Size von 1 und führt nach jeder Berechnung die Backpropagation durch, wodurch kein Mittelwert bei der Loss-Funktion berechnet wird. $\hat{y}$ sowie $\hat{y}_{adv}$ beziehen sich auf das aktuelle Bild im Algorithmus.

Für $\epsilon$ wird ein kleiner Wert wie $10^{-6}$ gewählt, der eine Division durch 0 verhindert, wenn der Output unseres Modells sowohl mit als auch ohne Perturbation gleich ist. Dies ist vor allem bei der Berechnung der ersten Perturbation ein Problem, da zu diesem Zeitpunkt ein Nulltensor vorhanden ist.

\subsubsection{Technische UAP Umsetzung} \label{chap:technische_umsetzung_uap}

Die technische Umsetzung des Prozesses zur Generierung der \acrshort{uap} Bilder ist im folgenden Pseudocode visualisiert.



\begin{algorithm}[H]
\caption{Algorithmus für die Generierung von \acrlong{uap}}
\begin{algorithmic}[1]
\label{algo:UAP Algorithmus}
\STATE \textbf{initialize} Liste $v$
\FOR{Anzahl Perturbationen $i$}
    \STATE \textbf{initialize} Nulltensor $v_i$
    \WHILE{$\text{Fooling Rate}(k) < r$}
        \FOR{$\text{img}$ in Trainingsdatensatz $X_{1:n}$}
        \STATE \textbf{initialize} Nulltensor $\Delta v$
        \STATE \textbf{initialize} $j_{\text{tries}} \gets 0$
            \WHILE{$(\hat{y}_{(img)} \geq k) = (\hat{y}_{({\text{img} + v_i + \Delta v})} \geq k)$ \textbf{and} $j_{\text{tries}} < t$}         
            \STATE Aktualisiere $\Delta v$ mittels $L_{\text{UAP}}$: $$\Delta v \gets \Delta v - lr_{\text{UAP}} (\frac{\partial L_{UAP}}{\partial \Delta v})$$
            \STATE Inkrementiere $j_{\text{tries}}$: $$j_{\text{tries}} \gets j_{\text{tries}} + 1$$
            \ENDWHILE
            \STATE Addiere die Perturbation $\Delta v$ auf $v_i$: $$v_i \leftarrow v_i + \Delta v$$
        \ENDFOR
    \ENDWHILE
    \STATE Füge $v_i$ zur Liste $v$ hinzu
\ENDFOR
\STATE \textbf{return} Liste von Perturbationen $v$
\end{algorithmic}
\end{algorithm}

\text{Wobei man folgende Parameter selbst bestimmen muss:}
\begin{align*}
i, &\text{ ist die Anzahl Perturbationsbilder, welche generiert werden.} \\
n, &\text{ ist die Anzahl Trainingsbilder, welche für die Generierung verwendet werden.} \\
t, &\text{ ist die Anzahl Versuche, eine bildlokale Perturbation zu finden.}\\
X_{1:n}, &\text{ ist eine Teilmenge aller positiven gelabelten Trainingsbilder.} \\
r, &\text{ ist der prozentuelle Anteil von Trainingsbilder, welche durch die Perturbationen} \\ 
&\text{  getäuscht werden, damit die Perturbation } v_i \text{ gespeichert wird.} \\
k, &\text{ ist der Threshold der Entscheidungsgrenze.} \\
p, &\text{ ist der Normparameter der $L_p$ Norm, siehe \nameref{Loss}.}\\
\lambda_{norm},\epsilon, &\text{ siehe \nameref{Loss}.}\\
lr_{\text{UAP}}, &\text{ ist die Learningrate für die Gewichtsaktualisierung von $\Delta v$.}
\end{align*}

Eine visuelle Darstellung zum \Gls{algorithmus} ist bei Abbildung \ref{fig:05-uap_algorithm} im Anhang zu finden. 

\subsubsection{Wahl des Matrizennormregularisierungsparameters $\lambda_{norm}$}

In Vorversuchen wurde festgestellt, dass jedes Modell standardmässig unterschiedlich robust gegenüber \acrshort{uap}s ist und dass der Matrizennormalisierungsparameter $\lambda_{norm}$ nicht universell gewählt werden kann. Wenn dieser Parameter zu hoch gewählt wird, wird die Perturbationsgenerierung für manche Modelle instabil und die gewünschte Schwelle der Entscheidungsgrenze $k$ kann nicht erreicht werden. Daher muss vor jeder Robustifizierung für jedes Modell manuell ein stabiler Regularisierungsparameter $\lambda_{norm}$ gefunden werden. Je höher dieser gewählt wird, desto instabiler ist die Generierung der \acrshort{uap}s, aber desto geringer ist die Perturbation.
\\
\\
In der folgenden Tabelle \ref{tab:Matrizennormregularisierungsparameter} werden einige der gewählten Werte erfasst:

\begin{table}[h!]
\centering
\begin{tabular}{l|l|l}
\hline
\textbf{Datensatz} & \textbf{Modell} & \textbf{$\lambda_{norm}$} \\ 
\hline
MRI & ResNet50 & $0.05$ \\
\hline
'' & ResNet18 & $>$1 \\
\hline
COVIDx & - & - \\
\hline
'' & - & - \\
\bottomrule
\end{tabular}
\caption{Beispiele der gewählten Matrizennormregularisierungsparameter $\lambda_{norm}$}
\label{tab:Matrizennormregularisierungsparameter}
\end{table}

\todo{Befüllen}