\subsection{Baseline Training}
\todo{Small introduction text}

\subsubsection{Loss-Funktion}
Für das Modelltraining verwenden wir den Binary-Cross-Entropy Loss. Die BCE Loss wird minimiert, wenn das Modell eine perfekte Vorhersage trifft (d.h. wenn $\hat{y}_i = y_i$ für alle i), und wird grösser, je weiter von entfernt ist).  \\
Die BCE Loss ist definiert als:

\begin{equation}
    L_{BCE} = -\frac{1}{N} \sum_{i=1}^{N} (y_i \cdot \log(\hat{y}_i) + (1-y_i) \cdot \log(1-\hat{y}_i))
    \label{eq:TrainingBCE}
\end{equation}

\text{folgende Parameter stehen für:}
\begin{align*}
y_i,        &\text{ tatsächliche Label des i-ten Datenpunkts} \\
\hat{y}_i,  &\text{ Vorhersage des Modells des i-ten Datenpunkts} \\
N,          &\text{ Gesamtanzahl an Datenpunkte} \\
\end{align*} 

\subsubsection{Optimizer}
Für die Optimierung der Modellparameter verwenden wir den Adam Optimizer von Kingma et al. \cite{kingma_adam_2017}. 
\todo{Muss ich genau Zittieren oder Paraphrasieren, vorallem bei der nachfolgenden Zusammenfassung?}
Adam ist ein Optimierungsalgorithmus, der für die Anpassung stochastischer Zielfunktionen mittels Gradientenverfahren entwickelt wurde. Er ist einfach zu implementieren, effizient und benötigt wenig Speicherplatz. Adam zeigt gute praktische Ergebnisse und ist mit anderen Methoden der stochastischen Optimierung vergleichbar. 

\subsubsection{Hyperparamteroptimierung und Modellselektion}
\todo{TODO}

\subsubsection{Metriken}
\todo{Ben}