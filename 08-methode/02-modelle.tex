\subsection{Klassifikationsmodelle} \label{chap:klassifikationsmodelle}
In dieser Thesis wurde die Generalisierbarkeit verschiedener Klassifikationsmodelle in unterschiedlichen Architekturen untersucht. Dazu wurden die durch die PyTorch Library bereitgestellt Modelle ResNet \cite{he_deep_2015}, DenseNet \cite{huang_densely_2018}, EfficientNetV2, \cite{tan_efficientnetv2_2021} AlexNet, \cite{krizhevsky_imagenet_2012}, VGG \cite{simonyan_very_2015} und der \acrfull{vit} \cite{dosovitskiy_image_2021} verwendet. In jeder Modellfamilie wurden zwei bis drei Modelle trainiert und evaluiert. Durch die Vielfalt der verwendeten Modelle wird sichergestellt, dass die Ergebnisse nicht auf eine spezifische Architektur beschränkt sind. Modelle, die kein erfolgreiches Training durchliefen, wurden aussortiert und nicht weiter berücksichtigt. Betroffen waren unter anderem die Modellfamilien AlexNet, VGG und der \acrfull{vit}. Eine ausführlichere Darstellung hierzu befindet sich im Kapitel \ref{chap:ergebnisse-modelltraining}.

Bei der Berechnung der Metriken, der Generation der Perturbationen und der \Gls{robustifizierung} des Modells, wird in dieser Arbeit jegliches Modell als $f(\cdot)$ referenziert.

\subsubsection{ResNet}
Das ResNet oder Residual Neural Network wurde von Kaiming et al. \cite{he_deep_2015} entwickelt und erreichte einen Fehler von 3.57\% auf dem ImageNet-Testdatensatz. Dieses Ergebnis führte zum Sieg in der Klassifikationsaufgabe des ILSVRC 2015. Beim ResNet werden Residual Connections eingesetzt, um sogenannte ``Skip Connections'' zwischen den Konvolutionen des neuronalen Netzwerks zu etablieren. Diese Verbindungen erleichtern den direkten Fluss von Informationen zwischen den Konvolutionen, was das Problem des Vanishing Gradient während des Trainings verhindert. Durch die Residual Connections können tiefe neuronale Netzwerke effektiver trainiert werden, da sie den Gradientenfluss durch das Netzwerk erleichtern.

\subsubsection{DenseNet}
Das DenseNet, kurz für Densely Connected Convolutional Networks, wurde von Huang et al. \cite{huang_densely_2018} eingeführt und stellt eine Weiterentwicklung des ResNet dar. Im Gegensatz zum ResNet, das Skip Connections zwischen bestimmten Konvolutionen herstellt, implementiert DenseNet eine dichte Verbindungsstruktur innerhalb sogenannter ``Dense Blocks''. In diesen Blöcken ist jede Konvolution direkt mit allen nachfolgenden Konvolutionen verbunden. Zwischen den Dense Blocks befinden sich Übergangsschichten, die für Dimensionsreduktion sorgen. Diese Struktur ermöglicht eine effiziente Informationsübertragung und fördert die Wiederverwendung von Merkmalen innerhalb des Netzwerks. Durch die direkten Verbindungen sowohl innerhalb als auch zwischen den Dense Blöcken kann DenseNet den Informationsverlust während des Trainings reduzieren und die Stabilität des Gradientenflusses verbessern.

\subsubsection{EfficientNetV2} 
EfficientNetV2, vorgestellt von Tan et al. \cite{tan_efficientnetv2_2021}, ist eine Weiterentwicklung des EfficientNet-Modells, die darauf abzielt, ein optimales Gleichgewicht zwischen Modellgrösse und Leistung durch eine skalierbare Compound Scaling-Strategie zu erreichen. Im Gegensatz zu früheren Ansätzen berücksichtigt EfficientNetV2 neben Tiefe, Breite und Auflösung auch die Netzwerkstruktur und verwendet verbesserte Bausteine wie EfficientConv, eine effizientere Variante der Standard-Konvolution. Diese Verbesserungen ermöglichen es dem Modell, mit weniger Parametern vergleichbare oder sogar bessere Leistungen bei verschiedenen Bilderkennungsaufgaben zu erzielen als andere Modelle. Die resultierende Effizienz macht EfficientNetV2 zu einer attraktiven Wahl für ressourcenbeschränkte Umgebungen oder Anwendungen, die schnelle Inferenzzeiten erfordern.
