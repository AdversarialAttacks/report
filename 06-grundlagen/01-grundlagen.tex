\section{Grundlagen, Stand der Forschung}
\subsection{Adversarial Attacks}

Adversarial Attacks sind ein faszinierendes Ph채nomen im Bereich des Deep Learning, das in den letzten Jahren zunehmend an Bedeutung gewonnen hat. Diese Angriffe beziehen sich auf gezielte Manipulationen von Eingabedaten, die darauf abzielen, neuronale Netzwerke zu t채uschen und falsche Vorhersagen zu erzwingen. Sie werfen wichtige Fragen zur Robustheit und Sicherheit von Deep-Learning-Modellen auf und haben weitreichende Implikationen f체r deren praktische Anwendungen. 
\todo{Erste Paper mit Perturbationen z.B. Paper von Goodfellow}

\subsubsection{Grundlagen}


\subsubsection{Allgemeine Beispiele f체r Adversarial Attacks}
\subsubsection{Adversarial Attacks auf Bildklassifikation}

\subsection{Universal Adversarial Attacks auf Bildklassifikation}
\subsubsection{Grundlagen}
\subsubsection{Unser Hauptpaper}
\subsubsection{Bisherige Herausforderungen bei der Verteidigung}